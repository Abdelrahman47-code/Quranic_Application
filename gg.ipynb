{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from Pytorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReciterCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=25600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Define the ReciterCNN class\n",
    "class ReciterCNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ReciterCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(32 * 16 * 50, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model and load weights\n",
    "num_classes = 12\n",
    "reciter_model = ReciterCNN(num_classes)\n",
    "reciter_model.load_state_dict(\n",
    "    torch.load(\"Quran_Reciters_Classification/model.pth\", map_location=\"cpu\", weights_only=True),\n",
    "    strict=False\n",
    ")\n",
    "reciter_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "def preprocess_audio_fixed_length(file_path, target_sample_rate=16000, n_mels=64, fixed_length=201):\n",
    "        \"\"\"Preprocess audio file into mel spectrogram for classification.\"\"\"\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=target_sample_rate, n_mels=n_mels)\n",
    "        mel_spec = mel_transform(waveform)\n",
    "        mel_spec_db = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "        if mel_spec_db.size(-1) < fixed_length:\n",
    "            mel_spec_db = torch.nn.functional.pad(mel_spec_db, (0, fixed_length - mel_spec_db.size(-1)))\n",
    "        else:\n",
    "            mel_spec_db = mel_spec_db[:, :, :fixed_length]\n",
    "        return mel_spec_db\n",
    "\n",
    "dummy_input = preprocess_audio_fixed_length(r\"D:/trying/Quranic_Application/Quran_Reciters_Classification/test_audios/001001.wav\")\n",
    "dummy_input = dummy_input.unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to Quran_Reciters_Classification/reciter_model.onnx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_path = \"Quran_Reciters_Classification/reciter_model.onnx\"\n",
    "torch.onnx.export(\n",
    "    reciter_model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "print(f\"Model exported to {onnx_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 6\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Function to predict using ONNX model\n",
    "def predict_onnx(mel_spec, onnx_model_path):\n",
    "    \"\"\"\n",
    "    Predict the class of the input mel-spectrogram using an ONNX model.\n",
    "\n",
    "    Args:\n",
    "        mel_spec (torch.Tensor): Input mel-spectrogram of shape (N, 1, H, W).\n",
    "        onnx_model_path (str): Path to the ONNX model file.\n",
    "    Returns:\n",
    "        int: Predicted class.\n",
    "    \"\"\"\n",
    "    if mel_spec.dim() != 4 or mel_spec.size(1) != 1:\n",
    "        raise ValueError(\"Input mel_spec must have shape (N, 1, H, W).\")\n",
    "\n",
    "    # Convert PyTorch tensor to NumPy array\n",
    "    mel_spec_np = mel_spec.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # Load the ONNX model\n",
    "    ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "    # Perform inference\n",
    "    inputs = {ort_session.get_inputs()[0].name: mel_spec_np}\n",
    "    outputs = ort_session.run(None, inputs)\n",
    "\n",
    "    # Extract the predicted class\n",
    "    predicted_class = np.argmax(outputs[0], axis=1)\n",
    "    return predicted_class[0]\n",
    "\n",
    "# Example usage\n",
    "onnx_model_path = \"Quran_Reciters_Classification/reciter_model.onnx\"\n",
    "mel_spec_example = preprocess_audio_fixed_length(\"D:/trying/Quranic_Application/Quran_Reciters_Classification/test_audios/001001.wav\").unsqueeze(0)\n",
    "predicted_class = predict_onnx(mel_spec_example, onnx_model_path)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: Quran_Reciters_Classification/test_audios\\002155.mp3 -> Quran_Reciters_Classification/test_audios\\002155.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\006155.mp3 -> Quran_Reciters_Classification/test_audios\\006155.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\008040.mp3 -> Quran_Reciters_Classification/test_audios\\008040.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\010065.mp3 -> Quran_Reciters_Classification/test_audios\\010065.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\010066.mp3 -> Quran_Reciters_Classification/test_audios\\010066.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\012030.mp3 -> Quran_Reciters_Classification/test_audios\\012030.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\018020.mp3 -> Quran_Reciters_Classification/test_audios\\018020.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\018050.mp3 -> Quran_Reciters_Classification/test_audios\\018050.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\028030.mp3 -> Quran_Reciters_Classification/test_audios\\028030.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\036020.mp3 -> Quran_Reciters_Classification/test_audios\\036020.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\043030.mp3 -> Quran_Reciters_Classification/test_audios\\043030.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\050030.mp3 -> Quran_Reciters_Classification/test_audios\\050030.wav\n",
      "Converted: Quran_Reciters_Classification/test_audios\\062009.mp3 -> Quran_Reciters_Classification/test_audios\\062009.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def convert_mp3_to_wav(directory):\n",
    "    \"\"\"\n",
    "    Converts all MP3 files in a directory to WAV format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing audio files.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            # Full path to the MP3 file\n",
    "            mp3_path = os.path.join(directory, filename)\n",
    "            # Generate the WAV file path\n",
    "            wav_path = os.path.splitext(mp3_path)[0] + \".wav\"\n",
    "            # Load the MP3 file\n",
    "            y, sr = librosa.load(mp3_path, sr=None)\n",
    "            # Save as WAV\n",
    "            sf.write(wav_path, y, sr, format='WAV')\n",
    "            print(f\"Converted: {mp3_path} -> {wav_path}\")\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"Quran_Reciters_Classification/test_audios\"  # Replace with your directory path\n",
    "convert_mp3_to_wav(directory_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
